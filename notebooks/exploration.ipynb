{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85517680-cdf1-4ce0-b3e4-aaf15e22a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- 1. DETECT LOCATION & SETUP PATHS ---\n",
    "# If running inside 'notebooks', save to the parent folder \"../src\"\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    print(f\"Detected execution inside '{os.getcwd()}'. Saving files to parent directory...\")\n",
    "    base_dir = \"../src\"\n",
    "else:\n",
    "    print(f\"Detected execution in root. Saving files to 'src/' directory...\")\n",
    "    base_dir = \"src\"\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# --- FILE 1: __init__.py ---\n",
    "init_code = \"\"\"\n",
    "from .etl import load_data\n",
    "from .feature_eng import engineer_features\n",
    "from .predictive_model import train_skip_model\n",
    "from .clustering import cluster_listeners\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"__init__.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(init_code)\n",
    "print(f\"Created: {base_dir}/__init__.py\")\n",
    "\n",
    "# --- FILE 2: etl.py ---\n",
    "etl_code = \"\"\"\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(raw_data_path):\n",
    "    print(f\"ETL Started: Looking for data in {raw_data_path}...\")\n",
    "    files = glob.glob(os.path.join(raw_data_path, \"*.json\"))\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {raw_data_path}. Please move your files there.\")\n",
    "    \n",
    "    data_frames = []\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data_frames.append(pd.DataFrame(data))\n",
    "            \n",
    "    df = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    if 'ts' in df.columns:\n",
    "        df['ts'] = pd.to_datetime(df['ts'])\n",
    "    \n",
    "    df['master_metadata_track_name'] = df['master_metadata_track_name'].fillna('Unknown Track')\n",
    "    df['master_metadata_album_artist_name'] = df['master_metadata_album_artist_name'].fillna('Unknown Artist')\n",
    "    \n",
    "    if 'ms_played' in df.columns and 'skipped' in df.columns:\n",
    "        df = df[(df['ms_played'] > 10000) | (df['skipped'] == True)]\n",
    "        \n",
    "    print(f\"âœ… ETL Complete. Processed {len(df)} rows.\")\n",
    "    return df\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"etl.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(etl_code)\n",
    "print(f\"âœ… Created: {base_dir}/etl.py\")\n",
    "\n",
    "# --- FILE 3: feature_eng.py ---\n",
    "feat_code = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def engineer_features(df):\n",
    "    print(\"âš™ï¸ Feature Engineering Started...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "    df['day_of_week'] = df['ts'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    if 'skipped' in df.columns:\n",
    "        df['is_skipped'] = df['skipped'].astype(int)\n",
    "    else:\n",
    "        df['is_skipped'] = 0\n",
    "        \n",
    "    if 'reason_start' in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df['reason_start_encoded'] = le.fit_transform(df['reason_start'].astype(str))\n",
    "        \n",
    "    if 'shuffle' in df.columns:\n",
    "        df['shuffle_feature'] = df['shuffle'].astype(int)\n",
    "        \n",
    "    return df\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"feature_eng.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(feat_code)\n",
    "print(f\"Created: {base_dir}/feature_eng.py\")\n",
    "\n",
    "# --- FILE 4: predictive_model.py ---\n",
    "model_code = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_skip_model(df):\n",
    "    print(\"Training Predictive Model (Random Forest)...\")\n",
    "    \n",
    "    features = ['ms_played', 'hour', 'day_of_week', 'is_weekend']\n",
    "    if 'reason_start_encoded' in df.columns:\n",
    "        features.append('reason_start_encoded')\n",
    "    if 'shuffle_feature' in df.columns:\n",
    "        features.append('shuffle_feature')\n",
    "        \n",
    "    X = df[features]\n",
    "    y = df['is_skipped']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Model Results - AUC Score: {auc_score:.2f}\")\n",
    "    return rf_model, auc_score, features\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"predictive_model.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(model_code)\n",
    "print(f\"Created: {base_dir}/predictive_model.py\")\n",
    "\n",
    "# --- FILE 5: clustering.py ---\n",
    "cluster_code = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cluster_listeners(df):\n",
    "    print(\"Running K-Means Clustering on Artists...\")\n",
    "    \n",
    "    artist_stats = df.groupby('master_metadata_album_artist_name').agg({\n",
    "        'ms_played': 'sum',\n",
    "        'is_skipped': 'mean',\n",
    "        'ts': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    artist_stats.columns = ['Artist', 'Total_Ms', 'Skip_Rate', 'Play_Count']\n",
    "    data = artist_stats[artist_stats['Play_Count'] > 20].copy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(data[['Skip_Rate', 'Play_Count']])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    data['Cluster'] = kmeans.fit_predict(X)\n",
    "    \n",
    "    print(\"Clustering Complete.\")\n",
    "    return data\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"clustering.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(cluster_code)\n",
    "print(f\"Created: {base_dir}/clustering.py\")\n",
    "\n",
    "print(\"\\n SUCCESS! All Python files have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caac275-7533-4bd2-b328-6c40d70e897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. Find the file\n",
    "raw_path = os.path.join('..', 'data', 'raw')\n",
    "files = glob.glob(os.path.join(raw_path, \"*.json\"))\n",
    "\n",
    "if files:\n",
    "    print(f\"found file: {files[0]}\")\n",
    "    # 2. Load just the first record to check columns\n",
    "    with open(files[0], 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    df_preview = pd.DataFrame(data)\n",
    "    print(\"\\n--- YOUR COLUMNS ARE: ---\")\n",
    "    print(list(df_preview.columns))\n",
    "else:\n",
    "    print(\"No files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39655b78-3b8a-4437-b47b-d3b143737031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- DETERMINE PATH ---\n",
    "# If running inside 'notebooks', save to '../src'\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    base_dir = \"../src\"\n",
    "else:\n",
    "    base_dir = \"src\"\n",
    "\n",
    "# --- 1. UPDATE ETL (Handle Column Names) ---\n",
    "etl_code = \"\"\"\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(raw_data_path):\n",
    "    print(f\"ETL Started: Looking for data in {raw_data_path}...\")\n",
    "    files = glob.glob(os.path.join(raw_data_path, \"*.json\"))\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No JSON files found in {raw_data_path}.\")\n",
    "    \n",
    "    data_frames = []\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data_frames.append(pd.DataFrame(data))\n",
    "            \n",
    "    df = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    # --- FIX FOR STANDARD DATA FORMAT ---\n",
    "    # Rename your columns to match the project standard\n",
    "    rename_map = {\n",
    "        \"endTime\": \"ts\",\n",
    "        \"artistName\": \"master_metadata_album_artist_name\",\n",
    "        \"trackName\": \"master_metadata_track_name\",\n",
    "        \"msPlayed\": \"ms_played\"\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "    \n",
    "    # Basic Formatting\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    df['master_metadata_track_name'] = df['master_metadata_track_name'].fillna('Unknown Track')\n",
    "    df['master_metadata_album_artist_name'] = df['master_metadata_album_artist_name'].fillna('Unknown Artist')\n",
    "    \n",
    "    print(f\"ETL Complete. Processed {len(df)} rows.\")\n",
    "    return df\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"etl.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(etl_code)\n",
    "print(\"Updated: src/etl.py (Compatible with your data)\")\n",
    "\n",
    "# --- 2. UPDATE FEATURE ENGINEERING (Create 'Skip' Logic) ---\n",
    "feat_code = \"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def engineer_features(df):\n",
    "    print(\"Feature Engineering Started...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Time Features\n",
    "    df['hour'] = df['ts'].dt.hour\n",
    "    df['day_of_week'] = df['ts'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # --- PROXY LABELING FOR RESUME ---\n",
    "    # Since 'skipped' column is missing, we assume:\n",
    "    # Play time < 30 seconds (30000ms) = SKIPPED\n",
    "    if 'skipped' not in df.columns:\n",
    "        df['is_skipped'] = df['ms_played'].apply(lambda x: 1 if x < 30000 else 0)\n",
    "    else:\n",
    "        df['is_skipped'] = df['skipped'].astype(int)\n",
    "        \n",
    "    # Standardize other columns if missing\n",
    "    if 'reason_start' in df.columns:\n",
    "        le = LabelEncoder()\n",
    "        df['reason_start_encoded'] = le.fit_transform(df['reason_start'].astype(str))\n",
    "        \n",
    "    return df\n",
    "\"\"\"\n",
    "with open(os.path.join(base_dir, \"feature_eng.py\"), \"w\", encoding='utf-8') as f:\n",
    "    f.write(feat_code)\n",
    "print(\"Updated: src/feature_eng.py (Added Skip Logic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a8dbf-218e-429d-bc27-9d1846c136f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib # <--- ADDED: To force reload of fixed modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Setup Path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# 2. Import Modules\n",
    "import src.etl\n",
    "import src.feature_eng\n",
    "import src.predictive_model\n",
    "import src.clustering\n",
    "\n",
    "# 3. FORCE RELOAD (Critical: Ensures the \"Fix\" you just made is actually used)\n",
    "importlib.reload(src.etl)\n",
    "importlib.reload(src.feature_eng)\n",
    "importlib.reload(src.predictive_model)\n",
    "importlib.reload(src.clustering)\n",
    "\n",
    "# 4. Import functions from reloaded modules\n",
    "from src.etl import load_data\n",
    "from src.feature_eng import engineer_features\n",
    "from src.predictive_model import train_skip_model\n",
    "from src.clustering import cluster_listeners\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "try:\n",
    "    print(\"Starting Pipeline...\")\n",
    "\n",
    "    # A. Load Data\n",
    "    raw_path = os.path.join('..', 'data', 'raw') \n",
    "    df = load_data(raw_path)\n",
    "    \n",
    "    # DEBUG: Check if column renaming worked\n",
    "    print(f\"Columns after ETL: {list(df.columns)[:5]}...\")\n",
    "    \n",
    "    # B. Feature Engineering\n",
    "    df_features = engineer_features(df)\n",
    "    print(f\"   âš™ï¸ Features Created. Skip Rate calculated: {df_features['is_skipped'].mean():.2%}\")\n",
    "    \n",
    "    # C. Run Predictive Model (Resume Validation)\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    rf_model, auc, features = train_skip_model(df_features)\n",
    "    print(f\"RESUME PROOF: Model AUC Score is {auc:.2f}\")\n",
    "    print(\"-----------------------------------\\n\")\n",
    "\n",
    "    # D. Run Clustering (Resume Validation)\n",
    "    clustered_df = cluster_listeners(df_features)\n",
    "    \n",
    "    # --- VISUALIZATION ---\n",
    "    \n",
    "    # Graph 1: Feature Importance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = range(len(importances))\n",
    "    plt.barh(indices, importances, align='center')\n",
    "    plt.yticks(indices, features)\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Why do I skip songs? (Feature Importance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Graph 2: Clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=clustered_df, x='Play_Count', y='Skip_Rate', hue='Cluster', palette='viridis', s=100)\n",
    "    plt.title('My Music Taste Clusters')\n",
    "    plt.xlabel('Play Count')\n",
    "    plt.ylabel('Skip Rate')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n ERROR: {e}\")\n",
    "    print(\"Check 'data/raw' folder!\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\n KEY ERROR: {e}\")\n",
    "    print(\"The ETL fix didn't apply. Please RESTART YOUR KERNEL (Kernel -> Restart) and try again.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a7f690-1f0b-4789-ae56-e7d3aba12c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ETL Started: Looking for data in ..\\data\\raw...\n",
      "âœ… ETL Complete. Processed 15222 rows.\n",
      "âš™ï¸ Feature Engineering Started...\n",
      "ğŸ¤– Spotify Data Agent Online.\n",
      "ğŸ“… Data Coverage: **May 2024** to **May 2025**\n",
      "Ready for specific dates (e.g., 'Nov 24 2023' or 'Saturdays in 2024').\n",
      "\n",
      "============================================================\n",
      "ğŸ¤– ADVANCED SPOTIFY AGENT IS LIVE!\n",
      "Try asking specific questions like:\n",
      "  ğŸ‘‰ 'What is Time listened in mins this (month, year)?'\n",
      "  ğŸ‘‰ 'Who is my top artist in (year)?'\n",
      "  ğŸ‘‰ 'Song count top 5 songs in Oct (year)'\n",
      "  ğŸ‘‰ 'When do I listen on (Day), (Month) (Year)?'\n",
      "  ğŸ‘‰ 'Top artist on 15th August 2024 (Specific Day)'\n",
      "(Type 'exit' to stop)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ YOU:  When do I listen on (mon), (june) (2024)?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AGENT: ğŸ•°ï¸ **Peak Listening Time (Mons June 2024):** Around **1 PM**.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ YOU:  When do I listen on (sat), (aug) (2024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AGENT: ğŸ•°ï¸ **Peak Listening Time (Sats August 2024):** Around **9 AM**.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ YOU:  When do I listen on (sat), (sep) (2024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AGENT: ğŸ•°ï¸ **Peak Listening Time (Sats September 2024):** Around **1 PM**.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ‘¤ YOU:  When do I listen on (sunday), (september) (2024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AGENT: ğŸ•°ï¸ **Peak Listening Time (Sundays September 2024):** Around **12 PM**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# 1. Setup Paths\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# 2. Import & FORCE RELOAD the Agent (Crucial step!)\n",
    "import agent.chat_bot\n",
    "importlib.reload(agent.chat_bot) \n",
    "\n",
    "from src.etl import load_data\n",
    "from src.feature_eng import engineer_features\n",
    "from agent.chat_bot import SpotifyAgent\n",
    "\n",
    "# 3. Load Data & Initialize\n",
    "# (We load data again to make sure everything is fresh)\n",
    "raw_path = os.path.join('..', 'data', 'raw')\n",
    "df = load_data(raw_path)\n",
    "df = engineer_features(df)\n",
    "\n",
    "# Initialize the NEW Agent\n",
    "my_agent = SpotifyAgent(df)\n",
    "\n",
    "# 4. RUN INTERACTIVE CHAT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¤– ADVANCED SPOTIFY AGENT IS LIVE!\")\n",
    "print(\"Try asking specific questions like:\")\n",
    "print(\"  ğŸ‘‰ 'What is Time listened in mins this (month, year)?'\")\n",
    "print(\"  ğŸ‘‰ 'Who is my top artist in (year)?'\")\n",
    "print(\"  ğŸ‘‰ 'Song count top 5 songs in Oct (year)'\")\n",
    "print(\"  ğŸ‘‰ 'When do I listen on (Day), (Month) (Year)?'\") \n",
    "print(\"  ğŸ‘‰ 'Top artist on 15th August 2024 (Specific Day)'\")\n",
    "print(\"(Type 'exit' to stop)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ğŸ‘¤ YOU: \")\n",
    "    \n",
    "    if user_input.lower() in ['exit', 'quit', 'stop']:\n",
    "        print(\"ğŸ‘‹ Bye!\")\n",
    "        break\n",
    "        \n",
    "    try:\n",
    "        response = my_agent.chat(user_input)\n",
    "        print(f\"ğŸ¤– AGENT: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c560a-7d24-4c2d-b2c9-fa5d09a9a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
